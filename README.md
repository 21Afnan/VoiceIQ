<div align="center">

# ğŸ™ï¸ VoiceIQ  
### **Voice-First Multi-Model AI Assistant**  

<p align="center">
  <strong><em>Talk to AI. Hear from many minds. One intelligent voice.</em></strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Status-Production Ready-success?style=flat-square&color=10B981"/>
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python"/>
  <img src="https://img.shields.io/badge/FastAPI-Backend-009688?style=flat-square&logo=fastapi"/>
  <img src="https://img.shields.io/badge/Multi--LLM-Orchestration-FF6B6B?style=flat-square"/>
  <img src="https://img.shields.io/badge/Voice--AI-STT%20%7C%20TTS-E91E63?style=flat-square"/>
</p>

<p align="center">
  <strong>
    Voice-First AI â€¢ RAG-Powered â€¢ Multi-LLM â€¢ Production-Ready
  </strong>
</p>

</div>

---

## ğŸŒŸ About VoiceIQ

**VoiceIQ** is a production-ready voice-first AI system that demonstrates modern best practices in AI application development:

- ğŸ¤ **Voice-Native Interaction** â€“ Speak naturally and hear AI respond  
- ğŸ§  **Multi-Model Intelligence** â€“ Get answers from 3 different AI models simultaneously  
- ğŸ“š **Knowledge-Aware (RAG)** â€“ AI understands domain-specific context via semantic search  
- âš¡ **Parallel Processing** â€“ Fast, concurrent LLM inference  
- ğŸ¨ **Beautiful UI** â€“ Modern, responsive frontend with gradient design  
- ğŸ—ï¸ **Clean Architecture** â€“ Scalable, maintainable, production-grade codebase  

> Perfect for portfolios, demos, startups, and AI research prototypes.

---

## âœ¨ Key Features

| Feature | Details |
|---------|---------|
| ğŸ¤ **Live Voice Recording** | Intuitive microphone-based interaction |
| ğŸ§ **Speech-to-Text** | Powered by Deepgram API (neural accuracy) |
| ğŸ§  **Intelligent Retrieval** | FAISS-based semantic search with sentence embeddings |
| ğŸ¤– **Triple LLM Power** | Google Gemini â€¢ DeepSeek â€¢ Moonshot Kimi |
| âš™ï¸ **Parallel Orchestration** | Concurrent inference (3 models simultaneously) |
| ğŸ”Š **Natural Speech Output** | Google Text-to-Speech with audio playback |
| ğŸ¨ **Responsive Design** | Mobile-friendly UI with gradient aesthetics |
| ğŸ” **Secure** | Environment-based secret management |
| ğŸ“Š **Production-Grade** | Error handling, timeouts, retry logic |

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERACTION                         â”‚
â”‚                  (Browser / Frontend)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“ (Audio File)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FRONTEND (Port 3000)                        â”‚
â”‚            Microphone Recording â€¢ Audio Processing           â”‚
â”‚         Vue/React-like UI with Gradient Design              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“ (POST /ask-voice)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FASTAPI BACKEND (Port 8000)                 â”‚
â”‚              Async Request Handling â€¢ CORS Setup            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“            â†“            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STT     â”‚ â”‚   RAG    â”‚ â”‚   LLMs   â”‚
    â”‚(Deepgram)â”‚ â”‚  (FAISS) â”‚ â”‚(Parallel)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚            â”‚            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   TTS + Audio Hex   â”‚
            â”‚   (Google gTTS)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“ (JSON Response)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Question + Answers + Audio   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Project Structure

```
VoiceIQ/
â”œâ”€â”€ ğŸ“„ README.md                 # You are here
â”œâ”€â”€ ğŸ”‘ .env                      # API keys (KEEP SECRET!)
â”œâ”€â”€ ğŸ“‹ requirements.txt          # Python dependencies
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ backend/                     # âš¡ FastAPI Backend
â”‚   â”œâ”€â”€ app.py                   # Main FastAPI application
â”‚   â”œâ”€â”€ config.py                # Configuration & API keys
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                    # Core AI Logic
â”‚   â”‚   â”œâ”€â”€ answer_engine.py     # LLM orchestrator (parallel inference)
â”‚   â”‚   â”œâ”€â”€ rag.py               # RAG retriever (FAISS + embeddings)
â”‚   â”‚   â””â”€â”€ ingest.py            # Data ingestion pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ llms/                    # LLM Integrations
â”‚   â”‚   â”œâ”€â”€ gemini.py            # Google Gemini API
â”‚   â”‚   â”œâ”€â”€ deepseek.py          # DeepSeek (via OpenRouter)
â”‚   â”‚   â”œâ”€â”€ kimi.py              # Kimi (via OpenRouter)
â”‚   â”‚   â””â”€â”€ prompts/
â”‚   â”‚       â””â”€â”€ prompt.txt       # System prompt template
â”‚   â”‚
â”‚   â”œâ”€â”€ voice/                   # Voice Processing
â”‚   â”‚   â”œâ”€â”€ stt.py               # Speech-to-Text (Deepgram)
â”‚   â”‚   â””â”€â”€ tts.py               # Text-to-Speech (Google gTTS)
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                    # Knowledge Base (RAG)
â”‚   â”‚   â”œâ”€â”€ chunks.json          # Vectorized text chunks
â”‚   â”‚   â”œâ”€â”€ embeddings.json      # Vector embeddings
â”‚   â”‚   â””â”€â”€ pages.json           # Source metadata
â”‚   â”‚
â”‚   â”œâ”€â”€ temp_audio/              # Runtime TTS output (auto-created)
â”‚   â””â”€â”€ tests/                   # Validation & integration tests
â”‚
â””â”€â”€ frontend/                    # ğŸ¨ Web UI
    â”œâ”€â”€ index.html               # Main page (1300+ lines)
    â””â”€â”€ config.js                # API config & utilities
```

---

## âš™ï¸ Technology Stack

### **Backend**
```
FastAPI (0.110.0)              Modern async Python web framework
Uvicorn                         ASGI server for FastAPI
Python 3.10+                    Core language
```

### **AI & Voice**
```
Google Gemini API               Latest generative AI model
OpenRouter                      Gateway for DeepSeek & Kimi
Deepgram (Nova-2)               Neural speech-to-text
Google Text-to-Speech (gTTS)    Voice synthesis
```

### **Vector Search & Embeddings**
```
FAISS (1.8.0+)                  Efficient similarity search
Sentence Transformers           Semantic embeddings (all-MiniLM-L6-v2)
NumPy                           Numerical computations
scikit-learn                    Additional ML utilities
```

### **Frontend**
```
HTML5                           Structure
CSS3 (Gradients)                Modern styling
JavaScript (Vanilla)            Client-side logic
Web Audio API                   Microphone access
Fetch API                       HTTP requests
```

---

## ğŸš€ Quick Start

### **Prerequisites**
- Python 3.10 or higher
- pip (Python package manager)
- Modern web browser (Chrome, Firefox, Safari, Edge)

### **Step 1: Clone Repository**
```bash
git clone https://github.com/your-username/voiceiq.git
cd voiceiq
```

### **Step 2: Create Virtual Environment**
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# macOS / Linux
python3 -m venv .venv
source .venv/bin/activate
```

### **Step 3: Install Dependencies**
```bash
pip install -r backend/requirements.txt
```

### **Step 4: Configure API Keys**
```bash
# Edit .env file and add your API keys:
GEMINI_API_KEY=your_key_here
DEEPSEEK_API_KEY=your_key_here
OPENROUTER_API_KEY=your_key_here
OPENROUTER_API_KEY2=your_key_here
DEEPGRAM_API_KEY=your_key_here
```

### **Step 5: Start Backend**
```bash
cd backend
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

Backend runs at: **http://localhost:8000**

### **Step 6: Start Frontend (New Terminal)**
```bash
cd frontend
python -m http.server 3000
```

Frontend runs at: **http://localhost:3000**

### **Step 7: Open in Browser**
Navigate to **http://localhost:3000** and start speaking! ğŸ¤

---

## ğŸ“¡ API Endpoints

### **Health Check**
```http
GET /
```
**Response:**
```json
{
  "status": "ok",
  "message": "VoiceIQ API running"
}
```

### **Process Voice**
```http
POST /ask-voice
Content-Type: multipart/form-data

Body: 
  file: <audio.wav>
```

**Response:**
```json
{
  "question_voice_text": "What curriculum does Sunmarke offer?",
  "answers_text": {
    "Gemini": "Sunmarke offers the British curriculum...",
    "DeepSeek": "The school provides multiple curriculum paths...",
    "Kimi": "Sunmarke School offers A-Level, IB, and BTEC..."
  },
  "answers_audio": {
    "Gemini": "48656c6c6f...",
    "DeepSeek": "48656c6c6f...",
    "Kimi": "48656c6c6f..."
  }
}
```

---

## ğŸ¯ Usage Guide

### **Recording & Processing**
1. **Click the microphone button** â€“ Starts recording
2. **Speak your question** â€“ Ask anything about the knowledge base
3. **Click again to stop** â€“ Ends recording and sends to backend
4. **Wait for responses** â€“ All 3 models respond in parallel (~5-10 seconds)
5. **Click voice icon** â€“ Hear each AI's answer read aloud

### **Example Questions**
- "What curriculum does Sunmarke offer?"
- "What are the school timings?"
- "What is the tuition fee?"
- "What extracurricular activities are available?"

---

## ğŸ§  How It Works

### **1. Speech-to-Text (STT)**
- Deepgram API transcribes your audio to text
- Neural model provides ~95%+ accuracy
- Supports multiple languages

### **2. Retrieval-Augmented Generation (RAG)**
- Converts your question to embeddings
- Searches FAISS index for similar content
- Retrieves top 5 relevant chunks from knowledge base
- Combines chunks into context for LLMs

### **3. Parallel LLM Inference**
- Question + Context sent to all 3 models simultaneously
- Uses Python's `ThreadPoolExecutor` for concurrency
- Models run in parallel (not sequential)
- Faster response time than waiting for each model

### **4. Text-to-Speech (TTS)**
- Google gTTS converts each AI response to MP3
- MP3 converted to hex string for JSON response
- Frontend decodes hex and plays audio

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| **STT Latency** | ~2-3 seconds (Deepgram) |
| **RAG Retrieval** | ~100ms (FAISS) |
| **LLM Response** | ~3-8 seconds (parallel) |
| **TTS Generation** | ~2-3 seconds per answer |
| **Total E2E** | ~10-15 seconds |
| **Concurrent Requests** | 3+ models in parallel |
| **Audio Format** | MP3 (hex-encoded in JSON) |

---

## ğŸ” Security & Environment

### **Required API Keys**
Create a `.env` file in the root directory:

```env
# Google Cloud
GEMINI_API_KEY=AIzaSy...

# OpenRouter (for DeepSeek & Kimi)
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_API_KEY2=sk-or-v1-...

# Deepgram
DEEPGRAM_API_KEY=4ade...
```

### **Best Practices**
- âœ… Never commit `.env` to version control
- âœ… Use `.gitignore` to exclude secrets
- âœ… Rotate API keys regularly
- âœ… Use service accounts for production
- âœ… Monitor API usage and costs

---

---

## âš ï¸ Limitations & Considerations

### **Current Limitations**
- STT requires **clear English speech**
- No audio file upload (must record in-app)
- Knowledge base is static (no real-time updates)
- Kimi availability depends on OpenRouter API status
- No user authentication (add for production)
- Single-threaded at API level (scale horizontally)

### **Future Enhancements**
- [ ] Custom knowledge base ingestion
- [ ] User authentication & API keys
- [ ] Conversation history & memory
- [ ] Custom LLM fine-tuning
- [ ] Real-time knowledge updates
- [ ] Advanced audio processing
- [ ] Multi-language support
- [ ] Deployment templates

---

## ğŸ¤ Contributing

This is a portfolio/demo project, but contributions are welcome!

### **Ways to Contribute**
- Report bugs and issues
- Suggest new features
- Improve documentation
- Optimize performance
- Add more LLM integrations
- Enhance UI/UX

---

## ğŸ“„ License

This project is open source under the **MIT License**.

---

## ğŸ™ Acknowledgments

- **FastAPI** â€“ Modern Python web framework
- **Deepgram** â€“ Neural speech-to-text
- **Google Cloud** â€“ Gemini & Text-to-Speech APIs
- **OpenRouter** â€“ LLM gateway service
- **FAISS** â€“ Vector similarity search
- **Sentence Transformers** â€“ Embedding models

---

<div align="center">

### **Built with â¤ï¸ for the AI community**

**VoiceIQ** demonstrates production-grade AI system design patterns.  
Perfect for learning, portfolios, and real-world applications.

**Version:** 1.0.0 | **Last Updated:** January 2026

</div>
